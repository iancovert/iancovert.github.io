<!DOCTYPE html>
<html lang="en">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-89623822-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-89623822-2');
  </script>
  <title>Choosing the right temperature for the Concrete distribution</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
  <style>
    body {
      font-family: "Georgia";
    }
    .navbar {
      padding-top: 26px;
      padding-bottom: 24px;
      padding-left: 5%;
      padding-right: 5%;
    }
    .navbar .nav-link {
      font-size: 1.25rem;
    }
    .navbar a {
      color: inherit;
    }
    .header-separator {
      margin-top: 0px;
    }
    .avatar {
      float: left;
      height: 48px;
      width: 48px;
      margin-right: 30px;
      margin-top: -8px;
    }
    a:hover {
      text-decoration: none;
    }
    .meta span {
      margin-right: 20px;
    }
    .meta a {
      text-decoration: none;
      color: inherit;
    }
    .content {
      margin-bottom: 100px;
    }
    .content h2 {
      margin-top: 32px;
      margin-bottom: 16px;
    }
    footer {
      padding-top: 15px;
      padding-bottom: 15px;
      background-color: #eee;
    }
    footer .footer-copyright a {
      text-decoration: none;
      color: inherit;
    }
    #accordion {
      margin-bottom: 30px;
    }
  </style>
</head>

<body>
  <!-- Header -->
  <nav class="navbar navbar-expand-lg">
    <div class="container-fluid">
      <a class="navbar-brand" href="/">
        <img src="/images/headshot.jpg" class="d-inline-block avatar">
        Ian Covert
      </a>

      <!-- TODO confusing behavior when page is too narrow -->
      <div class="collapse navbar-collapse">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/blog">Blog</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <hr class="header-separator">

  <!-- Content -->
  <div class="container content">
    <div class="row justify-content-md-center">
      <div class="col-lg-8">
        <h1>Choosing the right temperature for the Concrete distribution</h1>

        <hr>
        <div class="meta">
          <span class="author">By <a href="">Ian Covert</a></span>
          <span class="date">June 2, 2020</span>
        </div>
        <hr>

        <p>The Concrete distribution is a really powerful tool for modern machine learning. We use it to learn the parameters of discrete distributions, but it's actually a continuous distribution on the probability simplex. The temperature parameter controls the sparsity of samples, and although I've been using the Concrete distribution for a while, I've never been certain that I'm using the right temperature value.</p>

        <p>So that's what I've tried to figure out for this post. I'll start by explaining the role of the temperature parameter through the Concrete's relationship with other random variables (mainly Gumbel and Generalized Inverse Gamma), and then I'll run some simulations that provide practical guidance on how to set it in your models.</p>

        <h2>An introduction to the Concrete distribution</h2>

        <p>Before saying what the Concrete distribution is, here's why it's so famous. Starting in 2013, a technique known as the <em>reparameterization trick</em> (explained <a href="https://gregorygundersen.com/blog/2018/04/29/reparameterization/">here</a>) became popular for learning distributions of random variables inside neural networks [1, 2]. Learning distributional parameters can be tricky, but these papers showed how to get an unbiased, low-variance gradient estimator using a clever sampling trick. There were other techniques before it, but the reparameterization trick's low variance made it work much better, especially for VAEs. But unfortunately, although it worked for a variety of distributions (Gaussian, Gamma, Weibull, etc.), it didn't work out of the box for discrete random variables.</p>
        <!-- And although the reparameterization trick is ubiquitous today, it didn't work out of the box for discrete random variables.</p> -->

        <p>Then the Concrete distribution showed how to make it work. The Concrete distribution, introduced by two papers in 2016, is a <em>continuous</em> relaxation for <em>discrete</em> random variables (hence the name <em>con-crete</em>) [3, 4]. When you think about discrete random variables, you probably think of a random variable with the possible values <script type="math/tex">\{1, 2, \ldots, n\}</script>, but that set is <a href="http://www.math.wustl.edu/~freiwald/310equivalentsets6.pdf">equivalent</a> to the set of <script type="math/tex">n</script> one-hot vectors in <script type="math/tex">\mathbb{R}^n</script>. And the Concrete distribution is basically a relaxed version of a distribution over one-hot vectors. Its support is on the <script type="math/tex">n-1</script> simplex</p>

        <script type="math/tex mode=display">\Delta^{n-1} = \{x \in [0, 1]^n: \sum_{i=1}^n x_i = 1\},</script>

        <p>whereas actual one-hot vectors lie at the vertices</p>

        <script type="math/tex mode=display">\mathrm{Vert}(\Delta^{n-1}) = \{x \in \{0, 1\}^n: \sum_{i=1}^n x_i = 1\}.</script>

        <p>The Concrete distribution over <script type="math/tex">n</script> indices has <script type="math/tex">n + 1</script> parameters. There are <script type="math/tex">n</script> unnormalized probabilities <script type="math/tex">\alpha_1, \ldots, \alpha_n \geq 0</script> that control how likely each index is to dominate the others, and there's a temperature parameter <script type="math/tex">\lambda > 0</script> that controls the sparsity of samples.</p>

        <p>Formally, the distribution is defined by its density function <script type="math/tex">p_{\alpha, \lambda}(x)</script>, which for <script type="math/tex">X \sim \mathrm{Concrete}(\alpha, \lambda)</script> can be written as</p>

        <script type="math/tex mode=display">p_{\alpha, \lambda}(x) = (n-1)! \lambda^{n-1} \prod_{i=1}^n \frac{\alpha_i x_i^{-\lambda - 1}}{\sum_{k=1}^n \alpha_k x_k^{-\lambda}}</script>

        <p>for <script type="math/tex">x \in \Delta^{n-1}</script> [3]. That may not seem very intuitive, but we'll see a simpler interpretation below.</p>

        <p>First, here's what the samples look like for different sets of parameters.</p>

        <figure class="figure">
          <img src="images/concrete_samples.svg" class="figure-img img-fluid">
          <figcaption class="figure-caption">Figure 1: Samples of <script type="math/tex">X \sim \mathrm{Concrete}(\alpha, \lambda)</script> for different values of <script type="math/tex">\lambda</script>, with <script type="math/tex">n = 10</script> and <script type="math/tex">\alpha_i = \frac{1}{10}</script>.</figcaption>
        </figure>

        <p>As you can see, the samples get more discrete/sparse as the temperature <script type="math/tex">\lambda</script> is set to lower values. And since we usually want discrete samples, the ability to set the temperature is a necessary feature for working with the Concrete distribution.</p>

        <p>To see why the temperature has this effect, we need to consider how the Concrete relates to other random variables.</p>

        <ul>
          <li><p>In terms of independent uniform samples <script type="math/tex">U_i \sim \mathrm{Uniform}(0, 1)</script>, the random variable <script type="math/tex">X \sim \mathrm{Concrete}(\alpha, \lambda)</script> has dimensions <script type="math/tex">X_1, \ldots, X_n</script> equal to</p>

          <script type="math/tex mode=display">X_i = \frac{e^{(\log \alpha_i  - \log (- \log U_i))/\lambda}}{\sum_{k=1}^n e^{(\log \alpha_k  - \log (- \log U_k))/\lambda}}</script>

          <p>This is how we typically sample from the Concrete distribution in practice. But admittedly, the expression above probably doesn't seem very intuitive.</p></li>

          <li><p>In terms of independent Gumbel samples <script type="math/tex">G_i \sim \mathrm{Gumbel}(0, 1)</script>, the random variable <script type="math/tex">X \sim \mathrm{Concrete}(\alpha, \lambda)</script> has dimensions <script type="math/tex">X_1, \ldots, X_n</script> equal to</p>

          <script type="math/tex mode=display">X_i = \frac{e^{(\log \alpha_i + G_i)/\lambda}}{\sum_{k=1}^n e^{(\log \alpha_k + G_k) / \lambda}}</script>

          <p>or <script type="math/tex">X = \mathrm{softmax}(\frac{\log \alpha_1 + G_1}{\lambda}, \ldots, \frac{\log \alpha_n + G_n}{\lambda})</script>. This is what gives the Concrete distribution its other name, the "Gumbel-softmax distribution" [4]. And you can see that a low temperature <script type="math/tex">\lambda</script> could amplify the differences between the softmax arguments.</p></li>

          <li><p>An interpretation that I find even simpler is that <script type="math/tex">X \sim \mathrm{Concrete}(\alpha, \lambda)</script> has dimensions <script type="math/tex">X_1, \ldots, X_n</script> equal to</p>

          <script type="math/tex mode=display">X_i = \frac{V_i}{\sum_{k=1}^n V_k}</script>

          <p>where <script type="math/tex">V_i \sim \mathrm{GenInvGamma}(\alpha_i^{-1/\lambda}, \lambda, \lambda)</script>. So each dimension of the Concrete represents the proportion of the sum of <script type="math/tex">n</script> independent Generalized Inverse Gamma random variables.</p></li></ul>

          <p>That last part about the Generalized Inverse Gamma variables makes the Concrete look a bit like the Dirichlet distribution (another distribution on the probability simplex), because the random variable <script type="math/tex">Y \sim \mathrm{Dirichlet}(\beta)</script> has dimensions <script type="math/tex">Y_1, \ldots, Y_n</script> equal to</p>

          <script type="math/tex mode=display">Y_i = \frac{W_i}{\sum_{k=1}^n W_k}</script>

          <p>where <script type="math/tex">W_i \sim \mathrm{Gamma}(\beta_i, \theta)</script>, or equivalently <script type="math/tex">W_i \sim \mathrm{GenGamma}(\theta, \beta_i, 1)</script>. So the Concrete and Dirichlet distributions are actually pretty similar: they're both proportions of <script type="math/tex">n</script> independent non-negative random variables, and both depend on the Gamma distribution. Table 1 provides a comparison between the Concrete and Dirichlet distributions, and it highlights how both can be understood in terms of other distributions.</p>

          <table class="table table-sm table-bordered">
            <caption>Table 1: Concrete distribution versus Dirichlet distribution.</caption>
            <thead class="thead-light">
              <tr>
                <th class="text-center"></td>
                <th class="text-center">Concrete</td>
                <th class="text-center">Dirichlet</td>
              </tr>
            </thead>
            <tr>
              <td class="align-middle text-center">Parameters</td>
              <td class="align-middle text-center"><script type="math/tex">\alpha_1, \alpha_2, \ldots, \alpha_n \geq 0</script><br><script type="math/tex">\lambda > 0</script></td>
              <td class="align-middle text-center"><script type="math/tex">\beta_1, \beta_2, \ldots, \beta_n \geq 0</script></td>
            </tr>
            <tr>
              <td class="align-middle text-center">Support</td>
              <td class="align-middle text-center"><script type="math/tex">\Delta^{n-1}</script> for <script type="math/tex">\lambda > 0</script><br><script type="math/tex">\mathrm{Vert}(\Delta^{n-1})</script> for <script type="math/tex">\lambda = 0</script></td>
              <td class="align-middle text-center"><script type="math/tex">\Delta^{n-1}</script></td>
            </tr>
            <tr>
              <td class="align-middle text-center">Proportion of</td>
              <td class="align-middle text-center"><script type="math/tex">V_i \sim \mathrm{GenInvGamma}(\alpha_i^{-1/\lambda}, \lambda, \lambda)</script></td>
              <td class="align-middle text-center"><script type="math/tex">W_i \sim \mathrm{Gamma}(\beta_i, \theta)</script></td>
            </tr>
            <tr>
              <td class="align-middle text-center">Softmax of</td>
              <td class="align-middle text-center"><script type="math/tex">P_i \sim \mathrm{Gumbel}(\frac{\log \alpha_i}{\lambda}, \frac{1}{\lambda})</script></td>
              <td class="align-middle text-center"><script type="math/tex">Q_i \sim \mathrm{LogGamma}(\beta_i, \theta)</script></td>
            </tr>
            <tr>
              <td class="align-middle text-center">Primary Uses</td>
              <td class="align-middle text-center">Learning discrete distributions</td>
              <td class="align-middle text-center">Conjugate prior<br>Topic modeling</td>
            </tr>
          </table>

          <p>The Concrete and Dirichlet have some similarities, but their differences are significant. Remark that the Gamma distribution used by the Dirichlet is well-behaved: <script type="math/tex">W_i</script> has finite mean, and all of its moments exist. By contrast, the Generalized Inverse Gamma is heavy-tailed: for low temperatures <script type="math/tex">\lambda</script>, the mean, variance, and higher modes of <script type="math/tex">V_i</script> are all undefined [5]. Intuitively, that means that empirical estimators for these quantities would never converge because the occasional <em>massive</em> samples would throw them off. So at low temperatures <script type="math/tex">\lambda \approx 0</script>, the Concrete distribution is much more likely to have individual samples <script type="math/tex">V_i</script> that make up a large proportion of the total <script type="math/tex">\sum_{k=1}^n V_k</script>, which leads to values of <script type="math/tex">X</script> near <script type="math/tex">\mathrm{Vert}(\Delta^{n-1})</script>.</p>

          <p>This perspective gives some intuition for why low temperatures have the effect of increasing the sparsity of Concrete samples. If you're interested, check out the derivation of these results below.</p>

        <div id="accordion">
          <div class="card">
            <div class="card-header" id="headingOne">
              <h5 class="mb-0">
                <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseOne" aria-expanded="false" aria-controls="collapseOne">
                  Derivation
                </button>
              </h5>
            </div>
            <div id="collapseOne" class="collapse" aria-labelledby="headingOne" data-parent="#accordion">
              <div class="card-body">
                <p>Consider the sequence of operations that we use to produe a Concrete sample using the Gumbel-softmax trick. For convenience, we'll use <script type="math/tex">A, B, C\ldots</script> to denote the sequence of random variables.</p>

                <ol>
                  <li>Given <script type="math/tex">A \sim \mathrm{Uniform}(0, 1)</script>, <script type="math/tex">B = - \log A</script> has distribution <script type="math/tex">B \sim \mathrm{Exponential}(1)</script>.</li>
                  <li><script type="math/tex">C = - \log B</script> has distribution <script type="math/tex">C \sim \mathrm{Gumbel}(0, 1)</script>.</li>
                  <li><script type="math/tex">D = \log \alpha + C</script> has distribution <script type="math/tex">D \sim \mathrm{Gumbel}(\log \alpha, 1)</script>.</li>
                  <li><script type="math/tex">E = e^{-D} = \frac{1}{\alpha}e^{- C} = \frac{1}{\alpha} B</script> has distribution <script type="math/tex">E \sim \mathrm{Exponential}(\alpha)</script>, or equivalently <script type="math/tex">E \sim \mathrm{Gamma}(1, \alpha^{-1})</script> (using the shape-scale parameterization).</li>
                  <li><script type="math/tex">F = E^{\frac{1}{\lambda}}</script> has distribution <script type="math/tex">F \sim \mathrm{GenGamma}(\alpha^{-1/\lambda}, \lambda, \lambda)</script>.</li>
                  <li><script type="math/tex">G = \frac{1}{F}</script> has distribution <script type="math/tex">G \sim \mathrm{GenInvGamma}(\alpha^{-1/\lambda}, \lambda, \lambda)</script>.</li>
                  <li>A Concrete random variable <script type="math/tex">X \sim \mathrm{Concrete}(\alpha, \lambda)</script> is therefore equal to

                  <script type="math/tex mode=display">X_i = \frac{G_i}{\sum_{k=1}^n G_k}</script>

                  <p>where <script type="math/tex">G_i \sim \mathrm{GenInvGamma}(\alpha_i^{-1/\lambda}, \lambda, \lambda)</script>. And if you're wondering about the relationships between all these different probability distributions, here's where you can learn more about them: <a href="https://en.wikipedia.org/wiki/Exponential_distribution#Related_distributionshttps://en.wikipedia.org/wiki/Exponential_distribution#Related_distributions">Exponential</a>, <a href="https://en.wikipedia.org/wiki/Gumbel_distribution#Related_distributions">Gumbel</a>, <a href="https://en.wikipedia.org/wiki/Gamma_distribution#Related_distributions">Gamma</a>, <a href="https://reference.wolfram.com/language/ref/GammaDistribution.html">Generalized Gamma</a>, <a href="https://reference.wolfram.com/language/ref/InverseGammaDistribution.html">Generalized Inverse Gamma</a>, <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution#Related_distributions">Dirichlet</a>.</p></li>
                </ol>
              </div>
            </div>
          </div>
        </div>

        <p>Now, let's talk about the tradeoff we face when choosing a temperature value. What happens when we choose a high temperature or a low temperature?</p>

        <p><strong>High temperature</strong></p>

        <p>In the limit <script type="math/tex">\lambda \rightarrow \infty</script>, samples from the Concrete distribution aren't like one-hot vectors at all. In fact, they're deterministically equal to <script type="math/tex">(\frac{1}{n}, \ldots, \frac{1}{n})</script>, with the mass spread evenly between the indices [4]. Formally, we would say that for <script type="math/tex">X_\lambda \sim \mathrm{Concrete}(\alpha, \lambda)</script> we have</p>

        <script type="math/tex mode=display">\lim_{\lambda \rightarrow \infty} X_\lambda = (\frac{1}{n}, \ldots, \frac{1}{n}).</script>

        <p>This seems intuitive when you think about the "Gumbel-softmax" sampling trick, because the large temperature wipes out any differences between the arguments to the softmax.</p>

        <p><strong>Low temperature</strong></p>

        <p>In the limit <script type="math/tex">\lambda \rightarrow 0</script>, the samples start to actually look like one-hot vectors. This was proved in Proposition 1c of [3], where the authors showed that</p>

        <script type="math/tex mode=display">P(\lim_{\lambda \rightarrow 0} X_i = 1) = \frac{\alpha_i}{\sum_{k=1}^n \alpha_k}.</script>

        <p>Again, the increasing sparsity of the samples seems obvious when you consider the role of the temperature in the softmax. And the fact that the probability of <script type="math/tex">X_i</script> dominating is equal to <script type="math/tex">\alpha_i / \sum_{k=1}^n \alpha_k</script> is the magic of the Gumbel-max trick (which predates the Gumbel-softmax trick by several decades).</p>

        <p><strong>An easy choice?</strong></p>

        <p>Given what we've just seen, this seems like an easy choice—we should use <script type="math/tex">\lambda \approx 0</script> to get more discrete samples, right? Unfortunately, there's a tradeoff: the original paper explains that low <script type="math/tex">\lambda</script> is necessary for discrete samples, while high <script type="math/tex">\lambda</script> is necessary for getting large gradients [3]. And we need large gradients to learn the parameters <script type="math/tex">(\alpha_1, \ldots, \alpha_n)</script>.</p>

        <figure class="figure">
          <img src="images/tradeoff.png" class="figure-img img-fluid">
          <figcaption class="figure-caption">Figure 2: The tradeoff with the temperature parameter <script type="math/tex">\lambda</script>.</figcaption>
        </figure>

        <p>To deal with the tradeoff, some papers have used a carefully chosen fixed value of <script type="math/tex">\lambda</script> [6], and others have annealed <script type="math/tex">\lambda</script> from a high value to a low value [7]. I actually like the idea of annealing, because you can pass through a region of high <script type="math/tex">\lambda</script> that's suitable for learning while still ending up with discrete samples. But to do that, we need to figure out a range of values that makes sense.</p>

        <h2>Choosing the right temperature</h2>

        <p>What does it mean to choose the <em>right</em> temperature value? We want a large enough temperature to enable learning, but we also want a low enough temperature to get discrete samples. That's why annealing from a high value to a low value is a nice idea, because we can get the best of both worlds. We know how the Concrete distribution behaves when <script type="math/tex">\lambda \rightarrow \infty</script> or <script type="math/tex">\lambda \rightarrow 0</script>, and we want to pass between these two regimes when we anneal <script type="math/tex">\lambda</script>. The question is where the transition occurs.</p>

        <p>To figure this out, I tried to derive a mathematical function to describe the sparsity of Concrete samples as a function of <script type="math/tex">\alpha</script> and <script type="math/tex">\lambda</script>. Sadly, this turned out to be very hard. My plan was to quantify the sparsity of single samples though their entropy, or the value of the maximum index, and then figure out what that quantity was in expectation. It turns out that the <em>inverse</em> of the maximum index <script type="math/tex">M^{-1} = (\max_i X_i)^{-1}</script> is easiest to characterize, but figuring out <script type="math/tex">\mathbb{E}[M^{-1}]</script> was <em>still</em> too hairy. Maybe someone else will figure it out one day.</p>

        <p>Until then, we can fall back on simulations. The idea is to consider different values of <script type="math/tex">n</script> and <script type="math/tex">\lambda</script> to see how the sparsity of Concrete samples changes. Of course we should also look at different values of <script type="math/tex">\alpha</script>, but it seems obvious that letting <script type="math/tex">\alpha = (\frac{1}{n}, \ldots, \frac{1}{n})</script> will lead to the most spread out samples, so we'll just look at that "balanced" setting, plus one "imbalanced" setting where <script type="math/tex">\alpha_1 = 0.9</script> and the rest are <script type="math/tex">\alpha_2 = \ldots = \alpha_n = \frac{0.1}{n - 1}</script>.</p>

        <p>We'll use two quantities to measure sparsity. First, we'll look at the average entropy of samples, <script type="math/tex">\mathbb{E}[H(x)]</script>, by which I mean the entropy in the discrete random variable <script type="math/tex">Y \sim \mathrm{Categorical}(x)</script>. And second, we'll look at the expected value of the max index, <script type="math/tex">\mathbb{E}[M] = \mathbb{E}[\max_i X_i]</script>.</p>

        <p>I estimated each of these quantities for 100 different temperature values using a million samples, and the results demonstrate exactly what we wanted to see. The transition between perfect spread and perfect sparsity happens between <script type="math/tex">\lambda = 10.0</script> and <script type="math/tex">\lambda = 0.01</script> for a wide range of numbers of indices (<script type="math/tex">n \in \{10, 100, 1{,}000, 10{,}000\}</script>).</p>

        <p>Figure 3 shows that the mean entropy starts out high and then goes to zero, and we start getting sparse samples without having to resort to extremely low temperatures. Even for large values of <script type="math/tex">n</script> and balanced parameters, the samples have almost no entropy with <script type="math/tex">\lambda = 0.1</script>. And when the parameters are imbalanced, we can get sparse samples with even higher temperatures.</p>

        <figure class="figure">
          <img src="images/mean_entropy.svg" class="figure-img img-fluid">
          <figcaption class="figure-caption">Figure 3: The mean entropy of samples from the Concrete distribution for different values of <script type="math/tex">n, \lambda</script>, with two settings of <script type="math/tex">\alpha</script>. Horizontal lines show the maximum possible entropy for each value of <script type="math/tex">n</script>.</figcaption>
        </figure>

        <p>Figure 4 shows the expected value of the max index <script type="math/tex">\mathbb{E}[\max_i X_i]</script>, and these simulations tell the same story. According to these results, it would be safer to set the temperature to <script type="math/tex">\lambda = 0.01</script> to get truly discrete samples when <script type="math/tex">\alpha</script> is balanced, but for imbalanced parameters we can get near exact sparsity with <script type="math/tex">\lambda = 0.1</script>.</p>

        <figure class="figure">
          <img src="images/mean_max.svg" class="figure-img img-fluid">
          <figcaption class="figure-caption">Figure 4: The mean max value of samples from the Concrete distribution for different values of <script type="math/tex">n, \lambda</script>, with two settings of <script type="math/tex">\alpha</script>. Horizontal lines show the minimum possible max value for each value of <script type="math/tex">n</script>.</figcaption>
        </figure>

        <p>These results show that if you want to start with a higher temperature to make learning easier, then there's no point in going higher than <script type="math/tex">\lambda = 10.0</script>. And if you want to reduce the temperature to get truly discrete samples, then <script type="math/tex">\lambda = 0.01</script> should be sufficient. Even for reasonably large values of <script type="math/tex">n</script>, annealing the temperature between <script type="math/tex">\lambda = 10.0</script> and <script type="math/tex">\lambda = 0.01</script> allows you to transition between the two regimes, so you can get the best of both worlds by taking advantage of larger gradients for learning while still ending up with discrete samples.</p>

        <h2>Conclusion</h2>

        <p>The Concrete distribution is a powerful tool because it lets us learn the parameters of discrete distributions with the reparameterization trick, although the low temperatures that are necessary for truly discrete samples can actually make learning harder. These simulations gave us a good idea of the right range of <script type="math/tex">\lambda</script> values to use during training, by showing where the transition occurs between perfect spread and perfect sparsity.</p>

        <p>According to the results in these simulations, multiplicatively annealing the temperature from <script type="math/tex">\lambda = 10.0</script> to <script type="math/tex">\lambda = 0.01</script> (as in the Concrete Autoencoders paper [7]) or even just setting <script type="math/tex">\lambda = 0.1</script> (the fixed value used by L2X [6]) should be effective both for learning, and for ultimately getting discrete samples. I know that's what I'll be doing from now on.</p>

        <h2>References</h2>

        <ol>
          <li>Diederik Kingma, Max Welling. "Auto-encoding Variational Bayes." International Conference on Learning Representations, 2013.</li>
          <li>Danilo Rezende, Shakir Mohamed, Daan Wierstra. "Stochastic Backpropagation and Approximate Inference in Deep Generative Models." International Conference on Machine Learning, 2014.</li>
          <li>Chris Maddison, Andriy Mnih, Yee Whye Teh. "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables." International Conference on Learning Representations, 2016.</li>
          <li>Eric Jang, Shixiang Gu, Ben Poole. "Categorical Reparameterization with Gumbel-Softmax." International Conference on Learning Representations, 2016.</li>
          <li>Leigh Halliwell. "Classifying the Tails of Loss Distributions." Casualty Actuarial Society E-Forum, Spring 2013.</li>
          <li>Jianbo Chen et al. "Learning to Explain: An Information-Theoretic Perspective on Model Interpretation." International Conference on Machine Learning, 2018.</li>
          <li>Abubakar Abid, Muhammad Fatih Balin, James Zou. "Concrete Autoencoders for Differentiable Feature Selection and Reconstruction." International Conference on Machine Learning, 2019.</li>
          <!-- <li>David Clark, Muhammad El-Taha. "Some Useful Properties of Log-Logistic Random Variables for Health Care Simulations." International Journal of Statistics in Medical Research, 2015.</li> -->
        </ol>

      </div>
    </div>
  </div>
  <footer class="page-footer">
    <div class="footer-copyright text-center py-3">© 2020 Copyright: 
      <a href="https://iancovert.com/">iancovert.com</a>
    </div>
  </footer>

  <!-- Render math with Katex -->
  <script>
    var scripts = document.getElementsByTagName("script");
    for (var i = 0; i < scripts.length; i++) {
      /* TODO: keep going after an individual parse error. */
      var script = scripts[i];
      if (script.type.match(/^math\/tex/)) {
        var text = script.text === "" ? script.innerHTML : script.text;
        var options = script.type.match(/mode\s*=\s*display/) ?
                      {displayMode: true} : {};
        script.insertAdjacentHTML("beforebegin",
                                  katex.renderToString(text, options));
      }
    }
    document.body.className += " math_finished";
  </script>
</body>