<!DOCTYPE html>
<html lang="en">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-89623822-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-89623822-2');
  </script>
  <title>Explaining ML models with SHAP and SAGE</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: "Roboto", sans-serif;
    }
    .author {
      padding-right: 50px;
    }
    .navbar {
      padding-top: 26px;
      padding-bottom: 24px;
      padding-left: 5%;
      padding-right: 5%;
    }
    .navbar .nav-link {
      font-size: 1.25rem;
    }
    .navbar a {
      color: inherit;
    }
    .header-separator {
      margin-top: 0px;
    }
    .avatar {
      float: left;
      height: 48px;
      width: 48px;
      margin-right: 30px;
      margin-top: -8px;
    }
    a:hover {
      text-decoration: none;
    }
    .meta span {
      margin-right: 20px;
    }
    .meta a {
      text-decoration: none;
      color: inherit;
    }
    .content {
      margin-bottom: 100px;
    }
    .content h2 {
      margin-top: 32px;
      margin-bottom: 16px;
    }
    #disqus_thread {
      margin-top: 125px;
    }
    footer {
      padding-top: 15px;
      padding-bottom: 15px;
      background-color: #eee;
    }
    footer .footer-copyright a {
      text-decoration: none;
      color: inherit;
    }
  </style>
</head>

<body>
  <!-- Header -->
  <nav class="navbar navbar-expand-lg">
    <div class="container-fluid">
      <a class="navbar-brand" href="/">
        <img src="/images/headshot.jpg" class="d-inline-block avatar">
        Ian Covert
      </a>

      <!-- TODO confusing behavior when page is too narrow -->
      <div class="collapse navbar-collapse">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/publications">Publications</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/blog">Blog</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <hr class="header-separator">

  <!-- Content -->
  <div class="container content">
    <div class="row justify-content-md-center">
      <div class="col-lg-8">
        <h1>Explaining machine learning models with SHAP and SAGE</h1>

        <hr>
        <div class="meta">
          <span class="author">By <a href="">Ian Covert</a></span>
          <span class="date">April 24, 2020</span>
        </div>
        <hr>

        <p>Machine learning (ML) models are complicated, particularly expressive models like decision forests and neural networks. No interpretability tool provides a perfect understanding of the model or a solution for all our needs, so we need different tools to understand our models in different ways. In this post we'll discuss <a href="https://arxiv.org/abs/1705.07874">SHAP</a> [1] and <a href="https://arxiv.org/abs/2004.00668">SAGE</a> [2], two game theoretic methods based on the Shapley value.</p>

        <p>Each method answers a specific type of question:</p>

        <ul>
          <li>SHAP answers the question <em>how much does each feature contribute to this individual prediction?</em></li>
          <li>SAGE answers the question <em>how much does the model depend on each feature overall?</em></li>
        </ul>

        <p>SHAP is a method for explaining individual predictions (<em>local</em> interpretability), whereas SAGE is a method for explaining the model's behavior across the whole dataset (<em>global</em> interpretability). Figure 1 shows how each method is used.</p>

        <figure class="figure">
          <img src="images/shap_vs_sage.png" class="figure-img img-fluid">
          <figcaption class="figure-caption">Figure 1: SHAP explains individual predictions while SAGE explains the model's performance.</figcaption>
        </figure>

        <p>There are many approaches for both local and global interpretability, but SHAP and SAGE are particularly appealing because they answer their respective questions in a mathematically principled fashion, thanks to the Shapley value. This post explains their foundation in cooperative game theory, because this aspect of the methods is new to many users.</p>

        <p>We'll start by reviewing the Shapley value, a concept from game theory that's essential for understanding SHAP and SAGE. We'll then see how SHAP and SAGE use Shapley values for model understanding, and finally we'll discuss how the two methods are connected.</p>

        <h2>Shapley values</h2>

        <p>Shapley values were developed by Lloyd Shapley in a 1953 paper [3] about assigning credit to players in a cooperative game. The paper was written in the field of game theory, so Shapley values actually have nothing to do with ML. We can illustrate the idea behind Shapley values using a scenario with no ML involved.</p>

        <p>Imagine a company with <script type="math/tex">d</script> employees who are represented by the set <script type="math/tex">D = \{1, 2, \ldots, d\}</script>. The company makes $1,000,000 a year in profits when all the employees do their job, and management wants to distribute the profits through bonuses (Figure 2). However, management wants to do so in a fair way, rewarding people in accordance with their contribution to the company's profits.</p>

        <figure class="figure">
          <img src="images/company.png" class="figure-img img-fluid">
          <figcaption class="figure-caption">Figure 2: Imaginary company with profits to distribute as bonuses.</figcaption>
        </figure>

        <p>Management at this company is very insightful, so they have a powerful tool for quantifying each employee's contribution: they know exactly how profitable the company would have been with any subset <script type="math/tex">S \subseteq D</script> of the employees doing their job. (People with normal jobs will point out that no real company has this information, but bear with me.) As a couple examples:</p>

        <ul>
          <li>With all the employees working (<script type="math/tex">S = D</script>) they make $1,000,000.</li>
          <li>If everyone stayed but they lost their CEO (<script type="math/tex">S = \{2, 3, \ldots, d\}</script>), they would make half as much money, $500,000.</li>
          <li>If they kept everyone except for a couple of new hires (<script type="math/tex">S = \{1, 2, \ldots, d - 2\}</script>), they would make almost as much profit, $950,000.</li>
          <li>If no one worked there (<script type="math/tex">S = \{\}</script>) they would make precisely zero dollars.</li>
        </ul>

        <p>These are just a couple examples, but the management team knows exactly how profitable the company would be with all <script type="math/tex">2^d</script> subsets of employees. Management then summarizes their knowledge with a profitability function</p>

        <script type="math/tex mode=display">w: \mathcal{P}(D) \mapsto \mathbb{R}</script>

        <p>where <script type="math/tex">\mathcal{P}(D)</script> represents the power set of <script type="math/tex">D</script> (i.e., all subsets of <script type="math/tex">D</script>). For any subset of employees <script type="math/tex">S \subseteq D</script>, the quantity <script type="math/tex">w(S)</script> is the amount of profit the company makes with the employees in <script type="math/tex">S</script> working.</p>

        <p>Armed with this knowledge, the management team holds a meeting to discuss giving bonuses <script type="math/tex">\phi_1(w), \phi_2(w), \ldots, \phi_d(w)</script> to each employee <script type="math/tex">1, 2, \ldots, d</script>. (Note that the bonuses <script type="math/tex">\phi_i(w)</script> depend on the profitability function <script type="math/tex">w</script>, because if <script type="math/tex">w</script> were different then the bonuses should change.) When the team discusses what it means to allocate bonuses fairly, they come up with the following goals:</p>

        <ol>
          <li>(Efficiency) The bonuses should add up to the difference between the profitability with all employees and the profitability with no employees, or <script type="math/tex">\sum_{i \in D} \phi_i(w) = w(D) - w(\{\})</script>.</li>
          <li>(Symmetry) If two employees <script type="math/tex">i, j</script> are interchangeable, so that their inclusion always yields the same result, with <script type="math/tex">w(S \cup \{i\}) = w(S \cup \{j\})</script> for all <script type="math/tex">S</script>, then their bonuses should be identical, or <script type="math/tex">\phi_i(w) = \phi_j(w)</script>.</li>
          <li>(Dummy) If employee <script type="math/tex">i</script> contributes nothing, so that their inclusion yields no change in profitability, with <script type="math/tex">w(S \cup \{i\}) = w(S)</script> for all <script type="math/tex">S</script>, then they should receive no bonus, or <script type="math/tex">\phi_i(w) = 0</script>.</li>
          <li>(Linearity) If the company's total profitability function <script type="math/tex">w</script> is viewed as the linear combination of separate profitability functions <script type="math/tex">w_1, w_2, \ldots, w_k</script> for each of the company's <script type="math/tex">k</script> businesses, or <script type="math/tex">w = c_1w_1 + c_2w_2 \ldots + c_kw_k</script>, then the bonus for each employee determined on the company level <script type="math/tex">\phi_i(w)</script> should equal the linear combination of the employee's bonuses determined on the level of individual businesses <script type="math/tex">\phi_i(w_j)</script>, so that

          <script type="math/tex mode=display">\phi_i(w) = c_1 \phi_i(w_1) + c_2 \phi_i(w_2) \ldots + c_k \phi_i(w_k).</script></li>
        </ol>

        <p>These properties seem reasonable, so the management team wants the bonuses <script type="math/tex">\phi_i(w)</script> to satisfy properties 1-4. The challenge is, it isn't immediately clear how to do this or if it's even possible.</p>

        <p>It turns out that there is a way, and the fair bonus for each employee <script type="math/tex">i \in D</script> is given by the following formula:</p>

        <script type="math/tex mode=display">\phi_i(w) = \frac{1}{d} \sum_{S \subseteq D \setminus\{i\}} \binom{d - 1}{|S|}^{-1} \big[w(S \cup \{i\}) - w(S)\big]</script>

        <p>That formula looks complicated, but to calculate the bonus for employee <script type="math/tex">i</script> we're basically considering the profitability increase from adding <script type="math/tex">i</script> to a subset <script type="math/tex">S</script>, which is represented by</p>

        <script type="math/tex mode=display">w(S \cup \{i\}) - w(S)</script>

        <p>and then taking a weighted average of this quantity across all possible subsets <script type="math/tex">S</script> that don't include <script type="math/tex">i</script> already, or <script type="math/tex">S \subseteq D \setminus \{i\}</script>. The weights are a bit complicated, but this exact weighting scheme yields bonuses that satisfy properties 1-4. (If you're interested in understanding the weighting scheme, it comes from enumerating all possible orderings of the <script type="math/tex">d</script> employees, and then averaging the profitability bump from adding employee <script type="math/tex">i</script> across all those orderings.)</p>

        <p>You may wonder where this formula comes from and how we know that it satisfies the properties listed above. The formula was the subject of Lloyd Shapley's famous paper from 1953 [1], where he showed that for any profitability function <script type="math/tex">w</script> (<em>cooperative game</em> in game theory), the values <script type="math/tex">\phi_i(w)</script> (the Shapley values of <script type="math/tex">w</script>) provide the <em>unique</em> way of assigning scores to each employee (<em>player</em> in game theory) that satisfy properties 1-4. He actually used a different set of properties, but subsequent work showed that there are multiple sets of properties (or <em>axioms</em>) that lead to the same result.</p>

        <p>The formula for <script type="math/tex">\phi_i(w)</script> given above is the Shapley value—it's a function of a set function <script type="math/tex">w</script>, and it provides a summary of the contribution of each player <script type="math/tex">i \in D</script> to the total profit <script type="math/tex">w(D)</script>. Thanks to Lloyd Shapley, the management team can allocate bonuses using the Shapley value and know that they've been fair to their employees (Figure 3).</p>

        <figure class="figure">
          <img src="images/bonuses.png" class="figure-img img-fluid">
          <figcaption class="figure-caption">Figure 3: Fair bonuses allocated to each employee using the Shapley values <script type="math/tex">\phi_i(w)</script>.</figcaption>
        </figure>

        <p>Hopefully this example has given you some intuition for Shapley values. We used the example of a company here, but Shapley values can be used to summarize contributions to any cooperative game <script type="math/tex">w: \mathcal{P}(D) \mapsto \mathbb{R}</script>. Next, we'll discuss how SHAP and SAGE use Shapley values to help understand ML models.</p>

        <h2>SHAP (SHapley Additive exPlanations)</h2>

        <p>Let's re-enter the world of ML and see how SHAP uses Shapley values to explain individual predictions.</p>

        <p>Consider a ML model <script type="math/tex">f</script> that predicts a response variable <script type="math/tex">y</script> given an input <script type="math/tex">x</script>, where <script type="math/tex">x</script> consists of individual features <script type="math/tex">(x^1, x^2, \ldots, x^d)</script>. We'll use uppercase symbols (e.g., <script type="math/tex">X</script>) to refer to random variables and lowercase symbols (e.g., <script type="math/tex">x</script>) to refer to values. SHAP is designed to explain why the model <script type="math/tex">f</script> makes the prediction <script type="math/tex">f(x)</script> given the input <script type="math/tex">x</script>.</p>

        <p>As a running example, let's consider a model used by a bank to assess the likelihood of loan repayment. To keep it simple let's say there are just five features: the loan amount, the amount in the customer's checking account, the customer's age, their residence type, and their job. The model <script type="math/tex">f</script> predicts the probability that the loan will be repaid.</p>

        <p>Let's also focus on a single customer. John is requesting a loan for $2,500, he has $12,000 in his checking account, he's a 23 year old startup employee, and he lives in an apartment. The model predicts <script type="math/tex">f(x) = 0.7</script>, or that there's only a 70% chance John will repay his loan. That's a bit low, and John wants to know if it's because of his age, his job, or something else.</p>

        <p>SHAP explains an individual prediction by assigning values <script type="math/tex">\phi_1, \phi_2, \ldots, \phi_d</script> to each feature <script type="math/tex">x^1, x^2, \ldots, x^d</script> that quantify the feature's influence on the prediction <script type="math/tex">f(x)</script> (Figure 4). More specifically, each feature's value represents a contribution to the amount by which <script type="math/tex">f(x)</script> deviates from the mean prediction <script type="math/tex">\mathbb{E}[f(X)]</script>. In the case of the loan assessment model, we can use SHAP to understand why John's probability of repayment <script type="math/tex">f(x)</script> seems lower (riskier) than the average customer's outcome <script type="math/tex">\mathbb{E}[f(X)]</script>.</p>

        <figure class="figure">
          <img src="images/shap_diagram.png" class="figure-img img-fluid">
          <figcaption class="figure-caption">Figure 4: SHAP assigns each feature a value that represents whether it pushes the prediction <script type="math/tex">f(x)</script> higher or lower. For John, the loan amount and the checking amount push the probability of repayment higher, while his age, residence type and job push it lower.</figcaption>
        </figure>

        <p>To illustrate how SHAP works in an intuitive way, we'll imagine that our model is a human—an expert who is very good at making accurate predictions but not so good at explaining their logic. Furthermore, we'll assume that the expert is willing to make predictions given any subset of features (without knowing a customer's age, for example).</p>

        <p>When the expert says that John seems risky, we might ask, "what if I hadn't told you that John works for a startup?" Perhaps the expert would say that John would seem more likely to repay. Or, we might ask, "what if I hadn't told you John's job, the fact that he's 23, or that he lives in an apartment?" Now the expert may say that John would seem much more likely to repay. By providing the expert with subsets of features, we can see whether holding out information moves the prediction higher or lower. This is a first step towards understanding the expert's logic.</p>

        <p>It's a bit tougher to replicate this with a ML model <script type="math/tex">f</script>. Models are brittle and usually require a fixed set of features, so we can't just remove features and ask for a prediction given <script type="math/tex">x^S</script> for some <script type="math/tex">S \subseteq D</script>. SHAP therefore proposes a way of letting the model accommodate missing features.</p>

        <p>SHAP defines the cooperative game <script type="math/tex">v_{f, x}</script> to represent a prediction given the features <script type="math/tex">x^S</script>, as</p>

        <script type="math/tex mode=display">v_{f, x}(S) = \mathbb{E}[f(X) \; | \; X^S = x^S].</script>

        <p>That means that the values <script type="math/tex">x^S</script> are known, but the remaining features are treated as a random variable <script type="math/tex">X^{\bar S}</script> (where <script type="math/tex">\bar S = D \setminus S</script>), and we take the mean prediction <script type="math/tex">f(X)</script> when the unknown values follow the conditional distribution <script type="math/tex">X^{\bar S} \; | \; X^S = x^S</script>.</p>

        <p>Given this convention for making predictions using subsets of features, we can apply the Shapley value to define each feature's contribution to the prediction <script type="math/tex">f(x)</script> using the Shapley values <script type="math/tex">\phi_i(v_{f, x})</script>. This is just like what we did with the imaginary company, except we're replacing employees with features and the company's profitability with the model's prediction. The features containing the most evidence for successful repayment will have large positive values <script type="math/tex">\phi_i(v_{f,x}) > 0</script>, while uninformative features will have small values <script type="math/tex">\phi_i(v_{f, x}) \approx 0</script>, and features containing evidence for a failure to repay will have negative values <script type="math/tex">\phi_i(v_{f, x}) < 0</script>. These Shapley values are known as SHAP values.</p>

        <p>That's SHAP in a nutshell. There's more to it in practice because calculating <script type="math/tex">\phi_i(v_{f, x})</script> is computationally challenging: in fact, one of the SHAP paper's biggest contributions was proposing fast approximations that improve on the algorithm from the IME paper a couple years earlier [1, 4]. We won't talk about approximation in detail here, but KernelSHAP is fast and model-agnostic [1] and TreeSHAP is even faster because it's specific to tree-based models [5].</p>

        <h2>SAGE (Shapley Additive Global importancE)</h2>

        <p>Now we'll see how SAGE applies Shapley values to provide a different kind of model understanding: this time we want to explain how much <script type="math/tex">f</script> relies on each feature <script type="math/tex">X^1, X^2, \ldots, X^d</script> overall (Figure 5).</p>

        <figure class="figure">
          <img src="images/sage_diagram.png" class="figure-img img-fluid">
          <figcaption class="figure-caption">Figure 5: SAGE assigns each feature a value that represent how much the feature contributes to <script type="math/tex">f</script>'s performance. The most important features have the highest values.</figcaption>
        </figure>

        <p>How do we figure out which features are most important to a model? To get some intuition for SAGE, let's go back to the human expert who makes good predictions but can't explain their logic.</p>

        <p>To understand which features the expert derives the most information from, we can run an experiment where we deprive them of information and see how much their prediction accuracy suffers. For example, we can take a large sample of customers and ask the expert to predict their probability of repayment given everything but the customer's age. Their accuracy will probably drop by a little bit. Or, we can ask the expert to predict the probability of repayment given everything except age, checking account balance and job. Now their accuracy should drop by a lot because they're being deprived of critical information.</p>

        <p>To apply the same logic to a ML model <script type="math/tex">f</script>, we must once again confront the problem that <script type="math/tex">f</script> requires a fixed set of features. We can use the same trick as above and deal with missing features using their conditional distribution <script type="math/tex">X^{\bar S} \; | \; X^S = x^S</script>. We can now define a cooperative game that represents the model's performance given subsets of features. Given a loss function <script type="math/tex">\ell</script> (e.g., MSE or cross entropy loss), the game <script type="math/tex">v_f</script> is defined as</p>

        <script type="math/tex mode=display">v_f(S) = - \mathbb{E}[\ell(\mathbb{E}[f(X) \; | \; X^S], Y)].</script>

        <p>For any subset <script type="math/tex">S \subseteq D</script>, the quantity <script type="math/tex">v_f(S)</script> represents <script type="math/tex">f</script>'s performance given the features <script type="math/tex">X^S</script>, and we have a minus sign in front of the loss so that lower loss (improved accuracy) increases the value <script type="math/tex">v_f(S)</script>.</p>

        <p>Now, we can use the Shapley values <script type="math/tex">\phi_i(v_f)</script> to quantify each feature's contribution to the model's performance. The features that are most critical for the model to make good predictions will have large values <script type="math/tex">\phi_i(v_f) > 0</script>, while unimportant features will have small values <script type="math/tex">\phi_i(v_f) \approx 0</script>, and only features that make the model's performance worse will have negative values <script type="math/tex">\phi_i(v_f) < 0</script>. These are SAGE values, and that's SAGE in a nutshell.</p>

        <p>The paper describes SAGE and its properties in more detail, and it also proposes an algorithm for estimating SAGE values efficiently [2]. On the theory side, the paper shows that SAGE provides insight about intrinsic properties of the data distribution (which might be called <em>explaining the data</em>, rather than <em>explaining the model</em>) and that SAGE unifies a number of existing feature importance methods. We touch on both points briefly below.</p>

        <p><strong>Explaining the data.</strong> SAGE is primarily a tool for model interpretation, but it can also be used to gain insight about intrinsic relationships in the data. For example, when SAGE is applied with the Bayes classifier <script type="math/tex">f(x) = p(y \; | \; x)</script> and cross entropy loss, SAGE values are equal to the Shapley values of the mutual information function <script type="math/tex">v_f(S) = I(Y; X^S)</script>. This is good news for users who use ML to learn about the world (e.g., which genes are associated with breast cancer) and who aren't as interested in models for their own sake.</p>

        <p><strong>Unifying global feature importance methods.</strong> The paper proposes a class of methods that provide additive approximations of the predictive power contained in subsets of features. The framework of <em>additive importance measures</em> connects a number of methods that were previously viewed as unrelated. For example, we can see how SAGE differs from permutation tests, one of the methods in the framework.</p>

        <p>Permutation tests, proposed by Leo Breiman for assessing feature importance in random forest models, calculate how much the model performance drops when each column of the dataset is permuted [6]. SAGE can be viewed as modified version of a permutation test:</p>

        <ul>
          <li>Instead of holding out one feature at a time, SAGE holds out larger subsets of features. (By only removing individual features, permutation tests may erroneously assign low importance to features with good proxies.)</li>
          <li>SAGE draws held out features from their conditional distribution <script type="math/tex">p(X^{\bar S} \; | \; X^S = x^S)</script> rather than their marginal distribution <script type="math/tex">p(X^{\bar S})</script>. (Using the conditional distribution simulates a feature's absence, whereas using the marginal breaks feature dependencies and produces unlikely feature combinations.)</li>
          <!-- <li>SAGE averages over a distribution of values for the held out features before calculating the loss. (This allows SAGE to connect with properties of the data distribution, like mutual information.)</li> -->
        </ul>

        <p>This is SAGE in a nutshell, and hopefully this helps you understand its foundation in game theory.</p>

        <h2>How SHAP and SAGE are related</h2>

        <p>SHAP and SAGE both use Shapley values, but since they answer fundamentally different questions about a model (local versus global interpretability), it isn't immediately clear whether their connection goes deeper.</p>

        <p>It turns out that there is no simple relationship between SAGE values <script type="math/tex">\phi_i(v_f)</script> and SHAP values <script type="math/tex">\phi_i(v_{f, x})</script>. However, SAGE values are related to a SHAP variant known as <em>LossSHAP</em> [5]. Instead of explaining an individual prediction using the cooperative game <script type="math/tex">v_{f, x}</script>, we can use a game <script type="math/tex">v_{f, x, y}</script> that represents the loss for an individual input-output pair <script type="math/tex">(x, y)</script> given subsets of features <script type="math/tex">x^S</script>:</p>

        <script type="math/tex mode=display">v_{f, x, y}(S) = - \ell(\mathbb{E}[f(X) \; | \; X^S = x^S], y)</script>

        <p>The Shapley values of this game <script type="math/tex">\phi_i(v_{f, x, y})</script> are called LossSHAP values, and they represent how much each feature contributes to the prediction's accuracy. Features that make the prediction more accurate have large values <script type="math/tex">\phi_i(v_{f, x, y}) > 0</script>, and features that make the prediction less accurate have negative values <script type="math/tex">\phi_i(v_{f, x, y}) < 0</script>.</p>

        <p>The SAGE paper shows that SAGE values are equal to the expectation of the LossSHAP values [2]. Mathematically, we have</p>

        <script type="math/tex mode=display">\phi_i(v_f) = \mathbb{E}_{XY}[\phi_i(v_{f, X, Y})].</script>

        <p>The connection is important because it shows that a global explanation can be obtained via many local explanations: SAGE values can be calculated by calculating LossSHAP values for the whole dataset and then taking the mean, as illustrated in Figure 6.</p>

        <figure class="figure">
          <img src="images/loss_shap_diagram.png" class="figure-img img-fluid">
          <figcaption class="figure-caption">Figure 6: SAGE values are equal to the mean LossSHAP value.</figcaption>
        </figure>

        <p>However, waiting for LossSHAP values to converge for hundreds or thousands of examples in the dataset is very slow. The SAGE paper therefore proposes a faster approximation algorithm that aims directly at a global explanation—an approach we'll refer to as <em>SAGE sampling</em>. With SAGE sampling, SAGE values can be calculated in the same amount of time as just a handful of individual LossSHAP values. Figure 7 shows a comparison of the convergence of LossSHAP values and SAGE values for a gradient boosting machine trained on the <a href="https://archive.ics.uci.edu/ml/datasets/South+German+Credit+%28UPDATE%29">German Credit dataset</a>, where a global explanation is calculated at the cost of just <script type="math/tex">{\approx}10</script> local explanations.</p>

        <figure class="figure text-center">
          <img src="images/sage_convergence.png" class="figure-img img-fluid">
          <figcaption class="figure-caption text-left">Figure 7: Convergence comparison between SAGE values and LossSHAP values for several individual examples. The global explanation from SAGE can be calculated at the cost of just 10 local explanations.</figcaption>
        </figure>

        <p>Table 1 provides a comparison of SHAP, LossSHAP and SAGE values. Each method is built on Shapley values, but they answer different questions about ML models.</p>

        <table class="table table-sm table-bordered">
          <caption>Table 1: Comparison of SHAP, LossSHAP and SAGE values.</caption>
          <thead class="thead-light">
            <tr>
              <th class="text-center"></td>
              <th class="text-center">SHAP</td>
              <th class="text-center">LossSHAP</td>
              <th class="text-center">SAGE</td>
            </tr>
          </thead>
          <tr>
            <td class="text-center">Type of explanation</td>
            <td class="text-center">Local</td>
            <td class="text-center">Local</td>
            <td class="text-center">Global</td>
          </tr>
          <tr>
            <td class="text-center">Cooperative game</td>
            <td class="text-center"><script type="math/tex">v_{f, x}</script></td>
            <td class="text-center"><script type="math/tex">v_{f, x, y}</script></td>
            <td class="text-center"><script type="math/tex">v_f</script></td>
          </tr>
          <tr>
            <td class="text-center">Feature values</td>
            <td class="text-center"><script type="math/tex">\phi_i(v_{f, x})</script></td>
            <td class="text-center"><script type="math/tex">\phi_i(v_{f, x, y})</script></td>
            <td class="text-center"><script type="math/tex">\phi_i(v_f)</script></td>
          </tr>
          <tr>
            <td class="align-middle text-center">Meaning</td>
            <td class="align-middle text-center">Contribution to <br>prediction <script type="math/tex">f(x)</script></td>
            <td class="align-middle text-center">Contribution to <br>accuracy of <script type="math/tex">f(x)</script></td>
            <td class="align-middle text-center">Contribution to <br><script type="math/tex">f</script>'s performance</td>
          </tr>
          <tr>
            <td class="align-middle text-center">Approximation</td>
            <td class="align-middle text-center">IME [4]<br>KernelSHAP [1]<br>TreeSHAP [5]</td>
            <td class="align-middle text-center">LossSHAP sampling [2]<br>TreeSHAP [5]</td>
            <td class="align-middle text-center">Mean LossSHAP value [2]<br>SAGE sampling [2]</td>
          </tr>
        </table>

        <h2>Conclusion</h2>

        <p>Hopefully this post has given you a better understanding of Shapley values, as well as SHAP, SAGE and LossSHAP. Shapley values are pretty easy to understand outside the ML context (like with our example of a company that wants to give fair bonuses), and it's natural to apply the same logic to ML models once we deal with the fact that models require fixed sets of features.</p>

        <p>These methods are easy to use, so you should check them out: SHAP's Github page is <a href="https://github.com/slundberg/shap">here</a> and SAGE's Github page is <a href="https://github.com/iancovert/sage">here</a>.</p>

        <p><strong>Summary of notation:</strong></p>

        <ul>
          <li><script type="math/tex">w(S)</script>: the company's profitability function given the employee subset <script type="math/tex">S</script>.</li>
          <li><script type="math/tex">v_{f, x}(S)</script>: the model <script type="math/tex">f</script>'s prediction given the features <script type="math/tex">x^S</script> (for SHAP values).</li>
          <li><script type="math/tex">v_f(S)</script>: the model <script type="math/tex">f</script>'s performance (negative loss) given the features <script type="math/tex">X^S</script> (for SAGE values).</li>
          <li><script type="math/tex">v_{f, x, y}(S)</script>: the negative loss of <script type="math/tex">f</script>'s prediction given <script type="math/tex">x^S</script> and label <script type="math/tex">y</script> (for LossSHAP values).</li>
        </ul>

        <h2>References</h2>

        <ol>
          <li>Scott Lundberg, Su-In Lee. "A Unified Approach to Interpreting Model Predictions." <em>Neural Information Processing Systems, 2017.</em></li>
          <li>Ian Covert, Scott Lundberg, Su-In Lee. "Understanding Global Feature Contributions With Additive Importance Measures." <em>Neural Information Processing Systems, 2020.</em></li>
          <li>Lloyd Shapley. "A Value for n-Person Games." <em>Contributions to the Theory of Games, 1953.</em></li>
          <li>Erik Strumbelj, Igor Kononenko. "An Efficient Explanation of Individual Classifications Using Game Theory." <em>Journal of Machine Learning Research, 2010.</em></li>
          <li>Scott Lundberg et al. "From Local Explanations to Global Understanding with Explainable AI for Trees." <em>Nature Machine Intelligence, 2020.</em></li>
          <li>Leo Breiman. "Random Forests." <em>Machine Learning, 2001.</em></li>
        </ol>

        <h2>Acknowledgements</h2>

        <p>All the icons used in my diagrams were made by <a href="https://www.flaticon.com/authors/freepik">Freepik</a> and are available at <a href="https://www.flaticon.com">www.flaticon.com</a>.</p>

      </div>
    </div>

    <!-- Disqus -->
    <div id="disqus_thread"></div>
                                
  </div>
  <footer class="page-footer">
    <div class="footer-copyright text-center py-3">© 2020 Copyright: 
      <a href="https://iancovert.com/">iancovert.com</a>
    </div>
  </footer>

  <!-- Disqus embedding script -->
  <script>
    var disqus_config = function () {
      this.page.url = 'https://iancovert.com';
      this.page.identifier = 'understanding-shap-sage';
    };
    (function() {
    var d = document, s = d.createElement('script');
    s.src = 'https://iancovert-8qhha6oeg3.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  <!-- Render math with Katex -->
  <script>
    var scripts = document.getElementsByTagName("script");
    for (var i = 0; i < scripts.length; i++) {
      /* TODO: keep going after an individual parse error. */
      var script = scripts[i];
      if (script.type.match(/^math\/tex/)) {
        var text = script.text === "" ? script.innerHTML : script.text;
        var options = script.type.match(/mode\s*=\s*display/) ?
                      {displayMode: true} : {};
        script.insertAdjacentHTML("beforebegin",
                                  katex.renderToString(text, options));
      }
    }
    document.body.className += " math_finished";
  </script>
</body>