<!DOCTYPE html>
<html lang="en">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-89623822-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-89623822-2');
  </script>
  <title>Reduced rank regression</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: "Roboto", sans-serif;
    }
    .author {
      padding-right: 50px;
    }
    .navbar {
      padding-top: 26px;
      padding-bottom: 24px;
      padding-left: 5%;
      padding-right: 5%;
    }
    .navbar .nav-link {
      font-size: 1.25rem;
    }
    .navbar a {
      color: inherit;
    }
    .header-separator {
      margin-top: 0px;
    }
    .avatar {
      float: left;
      height: 48px;
      width: 48px;
      margin-right: 30px;
      margin-top: -8px;
    }
    a:hover {
      text-decoration: none;
    }
    .meta span {
      margin-right: 20px;
    }
    .meta a {
      text-decoration: none;
      color: inherit;
    }
    .content {
      margin-bottom: 100px;
    }
    .content h2 {
      margin-top: 32px;
      margin-bottom: 16px;
    }
    #disqus_thread {
      margin-top: 125px;
    }
    footer {
      padding-top: 15px;
      padding-bottom: 15px;
      background-color: #eee;
    }
    footer .footer-copyright a {
      text-decoration: none;
      color: inherit;
    }
    #accordion {
      margin-bottom: 30px;
    }
  </style>
</head>

<body>
  <!-- Header -->
  <nav class="navbar navbar-expand-lg">
    <div class="container-fluid">
      <a class="navbar-brand" href="/">
        <img src="/images/headshot2.jpg" class="d-inline-block avatar">
        Ian Covert
      </a>

      <!-- TODO confusing behavior when page is too narrow -->
      <div class="collapse navbar-collapse">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/publications">Publications</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/blog">Blog</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <hr class="header-separator">

  <!-- Content -->
  <div class="container content">
    <div class="row justify-content-md-center">
      <div class="col-lg-8">
        <h1>Reduced rank regression: simple multi-task learning</h1>

        <hr>
        <div class="meta">
          <span class="author">By <a href="">Ian Covert</a></span>
          <span class="date">September 1, 2022</span>
        </div>
        <hr>

        <p>Multi-task learning is a common approach in deep learning where we train a single model to perform multiple tasks. If the tasks are related, a single model may yield better performance than separate models trained for the individual tasks. For example, <a href="https://www.nature.com/articles/s41467-021-25680-7">this paper</a> trains a single deep learning model to predict multiple sparsely observed phenotypes related to Alzheimer's disease.</p>

        <p>Why does multi-task learning work? The intuitive argument is that similar tasks require similar internal representations (e.g., a view of a patient's Alzheimer's disease progression), so training a single model with the combined datasets is basically like training your model with more data. I imagine someone has made a more precise version of this argument, but I bet it's difficult because it's usually hard to prove anything about neural network training dynamics.</p>

        <p>Anyway, I was thinking about when we can more easily prove things about multi-task learning, and I wondered if there's a simpler version involving linear models. It turns out that there is, and it's called <em>reduced rank regression</em> [1, 2]. I'll introduce the problem here and then provide a derivation for optimal low-rank model, which fortunately has a closed-form solution.</p>

        <h2>Background: multivariate regression</h2>

        <p>First, a quick clarification: <em>multiple regression</em> refers to problems with multiple input variables, and <em>multivariate regression</em> refers to problems with multiple response variables (see <a href="https://en.wikipedia.org/wiki/Linear_regression">Wikipedia</a>). Here, we're going to assume both multiple inputs and multiple responses, but I'll refer to it as multivariate regression.</p>

        <p>Assume that we have the following data: a matrix of input features <script type="math/tex">X \in \mathbb{R}^{n \times d}</script> and a matrix of response variables <script type="math/tex">Y \in \mathbb{R}^{n \times p}</script>. As an example, <script type="math/tex">X</script> could represent the expression levels of <script type="math/tex">d</script> genes measured in <script type="math/tex">n</script> patients, and <script type="math/tex">Y</script> could represent <script type="math/tex">p</script> indicators of a disease's progression.</p>

        <p>A natural goal is to model <script type="math/tex">Y</script> given <script type="math/tex">X</script>, either for the purpose of providing predictions or to learn about the relationship between the inputs and responses. This is a multivariate regression problem, so we can fit a linear model that predicts all the labels simultaneously. Whereas multiple linear regression requires only one parameter per covariate, here we need a parameter matrix <script type="math/tex">C \in \mathbb{R}^{d \times p}</script>.</p>

        <p>To fit our model, we'll use the standard squared error loss function, which we can express here using the Frobenius norm:</p>

        <script type="math/tex mode=display">\mathcal{L}(C) = ||Y - XC||_F^2.</script>

        <p>If we don't constrain the model parameters, the optimal solution is straightforward to derive. Calculating the derivative and setting it to zero, we arrive at the following solution:</p>

        <script type="math/tex mode=display">\hat{C} = \arg \min_C \mathcal{L}(C) = (X^\top X)^{-1} X^T Y.</script>

        <p>Let's call this the ordinary least squares (OLS) solution. Why not stop here, aren't we done? The problem is that this is equivalent to fitting separate models for each column of the <script type="math/tex">Y</script> matrix, so it ignores the multivariate aspect of the problem. It doesn't leverage the similarity between the tasks, and furthermore, it doesn't help handle high-dimensional scenarios where <script type="math/tex">d, p > n</script>.</p>

        <p>There are many ways of dealing with this problem. I read Ashin Mukherjee's thesis [3] while writing this post, and his background section discusses several options: these include penalizing <script type="math/tex">C</script> (e.g., with ridge, lasso, group lasso or sparse group lasso penalties), and fitting the regression on a set of linearly transformed predictors (principal components regression, partial least squares, canonical correlation analysis). We'll focus on a different approach known as <em>reduced rank regression</em>, which requires the matrix <script type="math/tex">C</script> to have low rank.</p>

        <h2>Reduced rank regression</h2>
        
        <p>In reduced rank regression (RRR), we aim to minimize the squared error <script type="math/tex">\mathcal{L}(C)</script> while constraining <script type="math/tex">C</script> to have rank lower than its maximum possible value, <script type="math/tex">\min(d, p)</script>. Note that in practice we'll often have <script type="math/tex">n > d > p</script>. Given a value <script type="math/tex">r < \min(d, p)</script>, our problem becomes:</p>
        
        <script type="math/tex mode=display">\min_C \mathcal{L}(C) \ \ \text{s.t.} \ \ \text{rank}(C) = r.</script>
        
        <p>Deriving the solution isn't straightforward, so we'll break it down into a couple steps. At a high level, the derivation proceeds as follows:</p>
        
        <ol>
            <li>Re-write the RRR loss in terms of the OLS solution</li>
            <li>Find a lower bound on the RRR loss by solving a low-rank matrix approximation problem</li>
            <li>Guess a solution to the RRR problem and show that it achieves the lower bound</li>
        </ol>
        
        <p>Let's jump in. First, we'll re-write the RRR objective function using the OLS solution. Due to some standard properties about OLS residuals, we can write the following:</p>
        
        <script type="math/tex mode=display">\mathcal{L}(C) = ||Y - XC||_F^2 = ||Y - X \hat{C}||_F^2 + ||X \hat{C} - XC||_F^2.</script>
        
        <div id="accordion">
          <div class="card">
            <div class="card-header" id="headingOne">
              <h5 class="mb-0">
                <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseOne" aria-expanded="false" aria-controls="collapseOne">
                  Proof
                </button>
              </h5>
            </div>
            <div id="collapseOne" class="collapse" aria-labelledby="headingOne" data-parent="#accordion">
              <div class="card-body">
                <p>Adding and subtracting the OLS predictions yields the following:</p>

                <script type="math/tex mode=display">||Y - XB||_F^2 = ||Y - X\hat{B}||_F^2 + ||X \hat{B} - XB||_F^2 + 2 (Y - X\hat {B})^\top (X \hat{B} - XB)</script>
                
                <p>Let's focus on the third term and split it into two pieces that we can analyze separately:</p>
                
                <script type="math/tex mode=display">(Y - X\hat {B})^\top (X \hat{B} - XB) = (Y - X\hat {B})^\top X \hat{B} - (Y - X\hat {B})^\top XB</script>
                
                <p>For the first term, intuitively, we know that OLS residuals (<script type="math/tex">Y - X \hat{B}</script>) are uncorrelated with the predictions <script type="math/tex">X \hat{B}</script>. We can thus show that the first term is equal to zero:</p>
                
                <script type="math/tex mode=display">(Y - X\hat {B})^\top X \hat{B} = Y^\top (I - X (X^\top X)^{-1} X^T) X (X^\top X)^{-1} X^T Y  = 0</script>
                
                <p>For the second term, intuitively, we know that the OLS residuals are uncorrelated with <script type="math/tex">X</script>. We can thus show that the second term is also equal to zero:</p>
                
                <script type="math/tex mode=display">(Y - X\hat {B})^\top XB = Y^\top (I - X (X^\top X)^{-1} X^T) XB = Y^\top (X - X)B = 0</script>
                
                <p>We therefore have the following:</p>
                
                <script type="math/tex mode=display">||Y - XB||_F^2 = ||Y - X\hat{B}||_F^2 + ||X \hat{B} - XB||_F^2</script>
              </div>
            </div>
          </div>
        </div>
        
        <p>Because only the second term depends on <script type="math/tex">C</script>, solving the RRR problem is equivalent to minimizing <script type="math/tex">||X \hat{C} - XC||_F^2</script>. To simplify this slightly, we can define <script type="math/tex">\hat{Y} = X \hat{C}</script> and aim to minimize the following:</p>
        
        <script type="math/tex mode=display">||\hat{Y} - XC||_F^2</script>
        
        <p>Next, we'll find a lower bound on this matrix norm. Consider our prediction matrix <script type="math/tex">XC \in \mathbb{R}^{n \times p}</script>, where we assume that <script type="math/tex">X</script> is full-rank, or <script type="math/tex">\text{rank}(X) = \min(n, d)</script>. In general, we can have <script type="math/tex">\text{rank}(XC)</script> as large as <script type="math/tex">\min(n, p)</script>, but due to our constraint <script type="math/tex">\text{rank}(C) = r</script>, we have <script type="math/tex">\text{rank}(XC) \leq r</script> (see the matrix product rank property <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Properties">here</a>). In minimizing <script type="math/tex">||\hat{Y} - XC||_F^2</script>, we can therefore consider the related problem <script type="math/tex">||\hat{Y} - Z||_F^2</script> where <script type="math/tex">Z \in \mathbb{R}^{n \times p}</script> is low-rank.</p>
        
        <p>Our new low-rank matrix approximation problem is the following:</p>
        
        <script type="math/tex mode=display">\min_Z ||\hat{Y} - Z||_F^2 \ \ \text{s.t.} \ \ \text{rank}(Z) = r</script>
        
        <p>The solution to this non-convex problem is given by suppressing all but the top <script type="math/tex">r</script> singular values of <script type="math/tex">\hat{Y}</script>. That is, given the SVD <script type="math/tex">\hat{Y} = U \Sigma V^T</script> where <script type="math/tex">\text{rank}(\hat{Y}) = t \leq \min(n, p)</script>, we can write</p>
        
        <script type="math/tex mode=display">\hat{Y} = \sum_{i = 1}^t \sigma_i u_i v_i^\top,</script>
        
        <p>and by setting all but the <script type="math/tex">r</script> largest singular values <script type="math/tex">\sigma_i</script> to zero we arrive at the following rank-<script type="math/tex">r</script> approximation:</p>
        
        <script type="math/tex mode=display">\hat{Y}_r = \sum_{i = 1}^r \sigma_i u_i v_i^\top.</script>
        
        <p>According to the Eckart-Young-Mirsky theorem, this is the solution to the problem above, or</p>
        
        <script type="math/tex mode=display">\hat{Y}_r = \arg \min_Z ||\hat{Y} - Z||_F^2 \ \ \text{s.t.} \ \ \text{rank}(Z) = r</script>
        
        <div id="accordion">
          <div class="card">
            <div class="card-header" id="headingTwo">
              <h5 class="mb-0">
                <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
                  Proof
                </button>
              </h5>
            </div>
            <div id="collapseTwo" class="collapse" aria-labelledby="headingTwo" data-parent="#accordion">
              <div class="card-body">
                <p>This result follows directly from the Eckart-Young-Mirsky theorem (see <a href="https://en.wikipedia.org/wiki/Low-rank_approximation#Basic_low-rank_approximation_problem">here</a>), which says that the optimal low-rank matrix approximation is given by setting all but the largest singular values to zero.</p>
              </div>
            </div>
          </div>
        </div>
        
        <p>Now, given that <script type="math/tex">XC</script> has rank of at most <script type="math/tex">r</script>, we have the following lower bound on the re-written RRR objective:</p>
        
        <script type="math/tex mode=display">||\hat{Y} - XC||_F^2 \geq ||\hat{Y} - \hat{Y}_r||_F^2</script>
        
        <p>Next, we'll show that there exists a rank-<script type="math/tex">r</script> matrix <script type="math/tex">C^*</script> that exactly reproduces the low-rank predictions <script type="math/tex">\hat{Y}_r</script>. Using the same SVD as above, we can make the following educated guess:</p>
        
        <script type="math/tex mode=display">C^* = \hat{C} (\sum_{i = 1}^r v_i v_i^\top)</script>
        
        <p>There are two things to note about this candidate solution <script type="math/tex">C^*</script>. First, we have <script type="math/tex">\text{rank}(C^*) \leq r</script>, so <script type="math/tex">C^*</script> does not exceed our low-rank constraint.</p>
        
        <div id="accordion">
          <div class="card">
            <div class="card-header" id="headingThree">
              <h5 class="mb-0">
                <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseThree" aria-expanded="false" aria-controls="collapseThree">
                  Proof
                </button>
              </h5>
            </div>
            <div id="collapseThree" class="collapse" aria-labelledby="headingThree" data-parent="#accordion">
              <div class="card-body">
                <p>We have <script type="math/tex">C^* = \hat{C} \sum_{i = 1}^r v_i v_i^\top</script> with <script type="math/tex">\text{rank}(\hat{C}) \leq \min(d, p)</script> and <script type="math/tex">\text{rank}(\sum_{i = 1}^r v_i v_i^\top) = r</script>.</p>
                
                <p>Thus, we have <script type="math/tex">\text{rank}(C^*) = \text{rank}(\hat{C} \sum_{i = 1}^r v_i v_i^\top) \leq \min(d, p, r) = r</script> (see matrix rank properties <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Properties">here</a>).</p>
                    
                <p>We can't guarantee <script type="math/tex">\text{rank}(C^*) = r</script> in general (consider the case with <script type="math/tex">Y = 0</script>, for example), but the upper bound <script type="math/tex">\text{rank}(C^*) \leq r</script> should satisfy us because it shows we won't exceed the maximum allowable rank.</p>
              </div>
            </div>
          </div>
        </div>
        
        <p>Second, <script type="math/tex">C^*</script> yields predictions exactly equal to <script type="math/tex">\hat{Y}_r</script>, or</p>
        
        <script type="math/tex mode=display">XC^* = \hat{Y}_r</script>

        <div id="accordion">
          <div class="card">
            <div class="card-header" id="headingFour">
              <h5 class="mb-0">
                <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseFour" aria-expanded="false" aria-controls="collapseFour">
                  Proof
                </button>
              </h5>
            </div>
            <div id="collapseFour" class="collapse" aria-labelledby="headingFour" data-parent="#accordion">
              <div class="card-body">
                <p>This result is just algebra:</p>

                <script type="math/tex mode=display">XC^* = X \hat{C} (\sum_{i = 1}^r v_i v_i^\top) = \hat{Y} (\sum_{i = 1}^r v_i v_i^\top) = (\sum_{i = 1}^\tau \sigma_i u_i v_i^\top) (\sum_{j = 1}^r v_j v_j^\top) = \sum_{i = 1}^r \sigma_i u_i v_i^\top = \hat{Y}_r</script>
              </div>
            </div>
          </div>
        </div>
        
        <p>Thanks to the above, we have shown that <script type="math/tex">XC^*</script> is the optimal low-rank prediction matrix, or</p>
        
        <script type="math/tex mode=display">||\hat{Y} - XC^*||_F^2 = ||\hat{Y} - \hat{Y}_r||_F^2 = \min_Z ||\hat{Y} - Z||_F^2 \ \ \text{s.t.} \ \ \text{rank}(Z) = r.</script>
        
        <p>This implies that <script type="math/tex">C^*</script> solves our original problem, or</p>
        
        <script type="math/tex mode=display">C^* = \arg \min_C \mathcal{L}(C) \ \ \text{s.t.} \ \ \text{rank}(C) = r</script>
        
        <p>We'll refer to <script type="math/tex">C^*</script> as the reduced rank regression (RRR) solution.</p>

        <h2>Calculating the RRR solution</h2>

        <p>To summarize, the derivation above shows that fitting a RRR model has three steps. We must first solve the unconstrained OLS problem, which gives us</p>

        <script type="math/tex mode=display">\hat{C} = (X^\top X)^{-1} X^T Y.</script>

        <p>Next, we must define the OLS predictions <script type="math/tex">\hat{Y} = X \hat{C}</script> and then find the SVD, or</p>

        <script type="math/tex mode=display">\hat{Y} = U \Sigma V^\top.</script>

        <p>Finally, the RRR solution is given by first calculating an intermediate matrix <script type="math/tex">\sum_{i = 1}^r v_i v_i^\top</script>, and then calculating</p>

        <script type="math/tex mode=display">C^* = \hat{C} (\sum_{i = 1}^r v_i v_i^\top).</script>

        <p>If we want to create a solution path using different rank values, we can re-use the OLS and SVD steps and simply calculate <script type="math/tex">C^* = \hat{C} (\sum_{i = 1}^r v_i v_i^\top)</script> for each rank <script type="math/tex">r</script>, which is very efficient.</p>

        <h2>Relationship with PCA</h2>

        <p>Earlier this post, I mentioned that there were several approaches for training a multivariate regression model on linearly transformed predictors. Is that what's going on here? Not exactly, it turns out we're instead fitting the model with <em>linearly transformed labels</em>.</p>
        
        <p>To see this, we can re-write the RRR solution as follows:</p>
        
        <script type="math/tex mode=display">C^* = (X^\top X)^{-1} X^\top Y (\sum_{i = 1}^r v_i v_i^\top) = (X^\top X)^{-1} X^\top \tilde{Y}.</script>
        
        <p>This shows that we're effectively fitting a standard OLS multivariate regression, but using the projected, low-rank label matrix <script type="math/tex">\tilde{Y} = Y (\sum_{i = 1}^r v_i v_i^\top)</script> instead of <script type="math/tex">Y</script>.</p>
        
        <p>So the relationship with PCA is not that RRR is performing PCA on <script type="math/tex">X</script> and then fitting the model; that's called <em>principal components regression</em> [4]. The relationship is instead that PCA is a special case of RRR, where we have <script type="math/tex">Y = X</script>.</p>
        
        <p>In that case, our problem becomes</p>
        
        <script type="math/tex mode=display">\min_C ||X - XC||_F^2 \ \ \text{s.t.} \ \ \text{rank}(C) = r,</script>
        
        <p>and the solution is <script type="math/tex">C = \sum_{i = 1}^r v_i v_i^\top</script>, where <script type="math/tex">V</script> comes from the SVD of <script type="math/tex">X</script> itself (because the OLS parameters are <script type="math/tex">\hat{C} = I</script> and the OLS predictions are <script type="math/tex">\hat{Y} = X</script>). See [2] for a further discussion about the relationship with CCA, and [5] for a unification of other component analysis methods via RRR.</p>
        
        <h2>A shared latent space</h2>
        
        <p>In multi-task deep learning, it's common to have a shared hidden representation <script type="math/tex">h = f(x)</script> from which the various predictions are calculated. Once <script type="math/tex">h</script> is calculated, the predictions are often calculated using separate functions <script type="math/tex">g_1(h), \ldots, g_p(h)</script>. It turns out that we have something similar happening in the RRR case.</p>
        
        <p>Consider the RRR solution <script type="math/tex">C^* \in \mathbb{R}^{d \times p}</script>, which has rank <script type="math/tex">r < \min(d, p)</script>. Due to its low rank, <script type="math/tex">C^*</script> is guaranteed to have a factorization given by</p>
        
        <script type="math/tex mode=display">C^* = AB,</script>
        
        <p>where <script type="math/tex">A \in \mathbb{R}^{d \times r}</script> and <script type="math/tex">B \in \mathbb{R}^{r \times p}</script> (see <a href="https://en.wikipedia.org/wiki/Rank_factorization">Wikipedia</a>). The factorization is not unique: it can be constructed in multiple ways, including using the SVD. Regardless, any such factorization means that we have a shared latent representation <script type="math/tex">h = A^\top x \in \mathbb{R}^r</script> when calculating the predictions, and the separate <script type="math/tex">g_i</script> functions are projections that use the column vectors of <script type="math/tex">B</script> (see Figure 1).</p>
        
        <figure class="figure text-center">
            <img src="images/rrr.png" class="figure-img img-fluid">
            <figcaption class="figure-caption text-left">Figure 1: Comparison of shared latent spaces in multi-task deep learning (left) and RRR (right).</figcaption>
        </figure>

        <h2>Conclusion</h2>

        <p>This post has only shown a derivation for the most basic version of reduced rank regression. I'm sharing it because I found it interesting, but there's a lot of follow-up work on this topic: there are obvious questions to ask beyond deriving the optimal model (e.g., can we get confidence intervals for the learned coefficients?), ways of modifying the problem with regularization or random projections, and distinct ways to leverage the multi-task structure in multivariate regression.</p>
        
        <p>As additional references, the low-rank regression idea was introduced by Anderson [1], Izenman [2] introduced the term reduced rank regression and derived new results, and overviews of subsequent work are provided by Mukherjee [3], Reinsel & Velu [6] and Izenman [7].</p>
        
        <p>More broadly, it's nice that there are parallels between techniques we use in deep learning and classical analogues built on linear models (Table 1). Understanding the linear approaches can help build intuition for why the non-linear versions work, and luckily for those of doing deep learning research today, many of the key results for classical methods were derived decades ago.</p>

        <table class="table table-sm table-bordered">
          <caption>Table 1: Deep learning techniques and their classical analogues.</caption>
          <thead class="thead-light">
            <tr>
              <th class="align-middle text-center">Linear version</th>
              <th class="align-middle text-center">Deep learning version</th>
              <th class="align-middle text-center">Objective function</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="align-middle text-center">Linear regression</td>
              <td class="align-middle text-center">Supervised deep learning</td>
              <td class="align-middle text-center"><script type="math/tex">\min \mathbb{E}[(y - f(x))^2]</script></td>
            </tr>
            <tr>
                <td class="align-middle text-center">PCA</td>
                <td class="align-middle text-center">Autoencoder</td>
                <td class="align-middle text-center"><script type="math/tex">\min \mathbb{E}[||x - g(f(x))||^2]</script></td>
            </tr>
            <tr>
                <td class="align-middle text-center">RRR</td>
                <td class="align-middle text-center">Multi-task deep learning</td>
                <td class="align-middle text-center"><script type="math/tex">\min \mathbb{E}[||y - g(f(x))||^2]</script></td>
            </tr>
          </tbody>
        </table>

        <h2>References</h2>

        <ol>
          <li>Theodore Wilbur Anderson. "Estimating Linear Restrictions on Regression Coefficients for Multivariate Normal Distributions." <em>Annals of Mathematical Statistics, 1951.</em></li>
          <li>Alan Izenman. "Reduced Rank Regression for the Multivariate Linear Model." <em>Journal of Multivariate Statistics, 1975.</em></li>
          <li>Ashin Mukherjee. "Topics on Reduced Rank Methods for Multivariate Regression." <em>University of Michigan Thesis, 2013.</em></li>
          <li>William Massey. "Principal Components Regression in Exploratory Statistical Research." <em>JASA, 1965.</em></li>
          <li>Fernando de la Torre. "A Least-Squares Framework for Component Analysis." <em>TPAMI, 2012.</em></li>
          <li>Gregory Reinsel and Raja Velu. "Multivariate Reduced Rank Regression: Theory and Applications." <em>Springer, 1998.</em></li>
          <li>Alan Izenman. "Modern Multivariate Statistical Techniques: Regression, Classification and Manifold Learning." <em>Springer, 2008.</em></li>
        </ol>

        <!-- Disqus -->
        <div id="disqus_thread"></div>

      </div>
    </div>

  </div>
  <footer class="page-footer">
    <div class="footer-copyright text-center py-3">© 2021 Copyright: 
      <a href="https://iancovert.com/">iancovert.com</a>
    </div>
  </footer>

  <!-- Disqus embedding script -->
  <script>
    var disqus_config = function () {
      this.page.url = 'https://iancovert.com/blog/reduced-rank-regression/';
      this.page.identifier = 'reduced-rank-regression';
    };
    (function() {
    var d = document, s = d.createElement('script');
    s.src = 'https://iancovert-8qhha6oeg3.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  <!-- Render math with Katex -->
  <script>
    var scripts = document.getElementsByTagName("script");
    for (var i = 0; i < scripts.length; i++) {
      /* TODO: keep going after an individual parse error. */
      var script = scripts[i];
      if (script.type.match(/^math\/tex/)) {
        var text = script.text === "" ? script.innerHTML : script.text;
        var options = script.type.match(/mode\s*=\s*display/) ?
                      {displayMode: true} : {};
        script.insertAdjacentHTML("beforebegin",
                                  katex.renderToString(text, options));
      }
    }
    document.body.className += " math_finished";
  </script>
</body>